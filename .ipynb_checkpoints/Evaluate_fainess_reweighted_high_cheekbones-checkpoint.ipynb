{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import data_utils.data_utils_celeba_pytorch5 as data_utils\n",
    "import data_utils.network_tuning_valid as network_tuning\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# Load functions\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\AlgorithmicFairness'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [73,60,3]\n",
    "# Root directory for dataset\n",
    "dataroot = 'C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\AlgorithmicFairness'\n",
    "# Paths to data\n",
    "VALID_PATH = dataroot + \"\\\\data\\\\valid.csv\"  \n",
    "TEST_PATH = dataroot + \"\\\\data\\\\test.csv\"  \n",
    "IMAGE_PATHS = \"C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\celebA_resize3\"\n",
    "TARGET_COL = 'Smiling' #'Smiling'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# valid data\n",
    "data_valid = data_utils.CelebADataset(VALID_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)\n",
    "\n",
    "# test data\n",
    "data_test = data_utils.CelebADataset(TEST_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('Data/list_attr_celeba.txt', sep=\" \")\n",
    "attributes.drop('Unnamed: 41',axis=1, inplace=True)\n",
    "partition = pd.read_csv('Data/list_eval_partition.txt', sep=\" \", header=None, names=['im_id','partition'])\n",
    "matched = attributes.set_index('im_id').join(partition.set_index('im_id')).replace(-1,0)\n",
    "test_df = matched[matched['partition']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()          #<-- Your code here.   \n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get label\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(batch['target']))\n",
    "\n",
    "# Function to get input\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(batch['image']))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pkl = 'run5_weighted_loss_df.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe with training metrics\n",
    "df = pd.read_pickle('models/aws_models/'+df_pkl) #e.g. pd.read_pickle('df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers                                                               2\n",
       "activations          [<function relu at 0x000001C2BB3A2E18>, <funct...\n",
       "conv_out_channels                                                   32\n",
       "kernel_size                                                          5\n",
       "conv_stride                                                          1\n",
       "maxpool                                                              2\n",
       "dropout                                                            0.2\n",
       "batchnorm                                                         True\n",
       "optimizer                                                         Adam\n",
       "learning_rate                                                    0.001\n",
       "weight_decay                                                      0.01\n",
       "batch_size                                                         128\n",
       "num_epochs                                                           5\n",
       "net                                               weighted_loss_model2\n",
       "train_loss           [1.2006077766418457, 1.786297082901001, 1.1757...\n",
       "train_accs           [0.5390625, 0.59375, 0.7265625, 0.6171875, 0.6...\n",
       "valid_train_loss     [3.0455222129821777, 0.276113361120224, 0.2820...\n",
       "valid_train_accs     [0.5164343118667603, 0.8901193141937256, 0.879...\n",
       "valid_loss                                                     0.19558\n",
       "valid_accs                                                     0.91876\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  2\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "print('Model: ', i)\n",
    "layers = df['layers']\n",
    "activations = df['activations']\n",
    "conv_out_channels = df['conv_out_channels']\n",
    "kernel_size = df['kernel_size']\n",
    "conv_stride = df['conv_stride']\n",
    "maxpool = int(df['maxpool'])\n",
    "dropout = df['dropout']\n",
    "batchnorm = df['batchnorm']\n",
    "optimizer = df['optimizer']\n",
    "learning_rate = df['learning_rate']\n",
    "weight_decay = df['weight_decay']\n",
    "batch_size = int(df['batch_size'])\n",
    "num_epochs = df['num_epochs']\n",
    "\n",
    "net = network_tuning.tune_architecture(layers, activations, IMAGE_SHAPE, conv_out_channels, kernel_size,conv_stride, maxpool, dropout, batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'run5_weighted_loss_model2.dms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (batch1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (batch2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout2d(p=0.2, inplace=False)\n",
       "  (l_out): Linear(in_features=11520, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load network, choose the model i\n",
    "net.load_state_dict(torch.load('models/aws_models/'+model,map_location=torch.device('cpu'))) # e.g. net.load_state_dict(torch.load('model1'))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for saving predictions\n",
    "test_predictions = test_df.astype(object)\n",
    "test_predictions.loc[:,'target'] = test_predictions.loc[:,'Smiling']\n",
    "test_predictions.drop(['Smiling','partition'],axis=1)\n",
    "test_predictions['output'] = np.nan\n",
    "test_predictions['labels_argmax'] = np.nan\n",
    "test_predictions['accuracy'] = np.nan\n",
    "test_predictions['criterion'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-5148559b16e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_test\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_gen_test\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mget_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mlabels_argmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mj\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\DTU\\Kandidat\\Deep\\AlgorithmicFairness\\data_utils\\network_tuning_valid.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_img)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mfeatures_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mfeatures_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculation predictions on the validation set\n",
    "batch_gen_test = DataLoader(data_test, batch_size, shuffle=True, num_workers=6)\n",
    "test_1_losses,test_1_accs,test_1_lengths,test_0_losses,test_0_accs,test_0_lengths = 0, 0, 0, 0, 0, 0\n",
    "test_predictions = test_predictions.astype(object)\n",
    "j=0\n",
    "with torch.no_grad():\n",
    "    for batch_test in batch_gen_test:\n",
    "        \n",
    "        output = net(**get_input(batch_test))\n",
    "        labels_argmax = torch.max(get_labels(batch_test), 1)[1]\n",
    "        j+=1\n",
    "        for i in range(len(batch_test['file'])):\n",
    "            #plt.figure()\n",
    "            #plt.imshow(np.transpose(vutils.make_grid(batch_valid['image'][i], padding=2,normalize=True).cpu(),(1,2,0)))\n",
    "            #plt.axis('off')\n",
    "            #plt.ioff()\n",
    "            #plt.show()\n",
    "            #print('True Label: ',labels_argmax[i]) \n",
    "            #print('Model output: ', output['out'][i])\n",
    "            #print('File name: ', batch_valid['file'][i])\n",
    "            test_predictions.at[batch_test['file'][i],'output'] = output['out'][i]\n",
    "            test_predictions.at[batch_test['file'][i],'labels_argmax'] = labels_argmax[i]\n",
    "            test_predictions.at[batch_test['file'][i],'accuracy'] = accuracy(output['out'][i:i+1],labels_argmax[i:i+1])\n",
    "            test_predictions.at[batch_test['file'][i],'criterion'] = criterion(output['out'][i:i+1],labels_argmax[i:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be equalt to validation accuracy in df\n",
    "np.mean(test_predictions['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating accuracies for all protected attributes\n",
    "attributes = matched.iloc[:,0:-1].drop('Smiling',axis=1).columns\n",
    "df_att = pd.DataFrame(index = attributes, columns=['test_1_loss','test_1_acc','test_0_loss','test_0_acc','test_1_FP','test_1_TP','test_0_FP','test_0_TP','test_1_FN','test_0_FN','test_1_TN','test_0_TN'])\n",
    "batch_gen_test = DataLoader(data_test, batch_size, shuffle=True, num_workers=6)\n",
    "for att in range(0,len(attributes)):\n",
    "    print(attributes[att])\n",
    "    \n",
    "    att_ = test_predictions.iloc[:][attributes[att]]\n",
    "    t_ = test_predictions.iloc[:]['target']\n",
    "    \n",
    "    att_1 = np.where(att_==1)\n",
    "    att_0 = np.where(att_==0)\n",
    "    \n",
    "    att_1_t_1 = np.where((att_==1) & (t_==1))\n",
    "    att_0_t_1 = np.where((att_==0) & (t_==1))\n",
    "    att_1_t_0 = np.where((att_==1) & (t_==0))\n",
    "    att_0_t_0 = np.where((att_==0) & (t_==0))\n",
    "    \n",
    "    df_att.iloc[att]['test_1_loss'] = np.mean(test_predictions.iloc[att_1]['criterion'])\n",
    "    df_att.iloc[att]['test_1_acc'] = np.mean(test_predictions.iloc[att_1]['accuracy'])\n",
    "    df_att.iloc[att]['test_0_loss'] = np.mean(test_predictions.iloc[att_0]['criterion'])\n",
    "    df_att.iloc[att]['test_0_acc'] = np.mean(test_predictions.iloc[att_0]['accuracy'])\n",
    "    df_att.iloc[att]['test_1_FP'] = 1-np.mean(test_predictions.iloc[att_1_t_0]['accuracy'])\n",
    "    df_att.iloc[att]['test_0_FP'] = 1-np.mean(test_predictions.iloc[att_0_t_0]['accuracy'])\n",
    "    df_att.iloc[att]['test_1_TP'] = np.mean(test_predictions.iloc[att_1_t_1]['accuracy'])\n",
    "    df_att.iloc[att]['test_0_TP'] = np.mean(test_predictions.iloc[att_0_t_1]['accuracy'])\n",
    "    df_att.iloc[att]['test_1_FN'] = 1-np.mean(test_predictions.iloc[att_1_t_1]['accuracy'])\n",
    "    df_att.iloc[att]['test_0_FN'] = 1-np.mean(test_predictions.iloc[att_0_t_1]['accuracy'])\n",
    "    df_att.iloc[att]['test_1_TN'] = np.mean(test_predictions.iloc[att_1_t_0]['accuracy'])\n",
    "    df_att.iloc[att]['test_0_TN'] = np.mean(test_predictions.iloc[att_0_t_0]['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att['TP_OR'] = df_att['test_1_TP'] /df_att['test_0_TP'] \n",
    "df_att['FP_OR'] = df_att['test_1_FP'] /df_att['test_0_FP']\n",
    "df_att['acc_ratio'] = df_att['test_1_acc'] /df_att['test_0_acc']\n",
    "df_att['FN_OR'] = df_att['test_1_FN'] /df_att['test_0_FN'] \n",
    "df_att['TP_OR'] = pd.to_numeric(df_att['TP_OR'])\n",
    "df_att['FP_OR'] = pd.to_numeric(df_att['FP_OR'])\n",
    "df_att['acc_ratio'] = pd.to_numeric(df_att['acc_ratio'])\n",
    "df_att['FN_OR'] = pd.to_numeric(df_att['FN_OR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att = df_att.sort_values('acc_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.DataFrame(index=['Smiling','Not Smiling'],columns=['Smiling','Not Smiling'])\n",
    "df_confusion.at['Smiling','Smiling'] = df_att.loc['High_Cheekbones']['test_0_TP']\n",
    "df_confusion.at['Smiling','Not Smiling'] = df_att.loc['High_Cheekbones']['test_0_FP']\n",
    "df_confusion.at['Not Smiling','Smiling'] = df_att.loc['High_Cheekbones']['test_0_FN']\n",
    "df_confusion.at['Not Smiling','Not Smiling'] = df_att.loc['High_Cheekbones']['test_0_TN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "cmap=sns.color_palette(\"RdBu\",70)\n",
    "heatmap_kwargs = dict(vmin=0, vmax=1, cmap=cmap,annot=True, linewidths=.5, fmt='.4g')\n",
    "ax = sns.heatmap(df_confusion.astype('float'), center=0.5, **heatmap_kwargs)\n",
    "ax.set_xticklabels(list(df_confusion.columns))\n",
    "ax.set_yticklabels(list(df_confusion.index))\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(model+'.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion2 = pd.DataFrame(index=['Smiling','Not Smiling'],columns=['Smiling','Not Smiling'])\n",
    "df_confusion2.at['Smiling','Smiling'] = df_att.loc['High_Cheekbones']['test_1_TP']\n",
    "df_confusion2.at['Smiling','Not Smiling'] = df_att.loc['High_Cheekbones']['test_1_FP']\n",
    "df_confusion2.at['Not Smiling','Smiling'] = df_att.loc['High_Cheekbones']['test_1_FN']\n",
    "df_confusion2.at['Not Smiling','Not Smiling'] = df_att.loc['High_Cheekbones']['test_1_TN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5,5))\n",
    "cmap=sns.color_palette(\"RdBu\",70)\n",
    "heatmap_kwargs = dict(vmin=0, vmax=1, cmap=cmap,annot=True, linewidths=.5, fmt='.4g')\n",
    "ax = sns.heatmap(df_confusion2.astype('float'), center=0.5, **heatmap_kwargs)\n",
    "ax.set_xticklabels(list(df_confusion2.columns))\n",
    "ax.set_yticklabels(list(df_confusion2.index))\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(model+'.png',dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att2 = df_att.loc[['Rosy_Cheeks','Gray_Hair','Bags_Under_Eyes','Bald','Chubby','Double_Chin','Goatee','High_Cheekbones','Young','Narrow_Eyes','Blond_Hair','Male','Arched_Eyebrows','Receding_Hairline','Attractive','Oval_Face','Big_Lips','Sideburns','Big_Nose']].sort_values('acc_ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_att2[['acc_ratio','FN_OR','FP_OR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "cmap=sns.color_palette(\"RdBu\",70)\n",
    "heatmap_kwargs = dict(vmin=0, vmax=4, cmap=cmap,annot=True, linewidths=.5, fmt='.4g')\n",
    "ax = sns.heatmap(df_att2[['acc_ratio','FN_OR','FP_OR']], center=1.0, **heatmap_kwargs)\n",
    "ax.set_xticklabels(['Accuracy ratio','False negative Odds Ratio','False Positive Odds Ratio'])\n",
    "ax.set_yticklabels(list(df_att2.index))\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.xaxis.set_ticks_position('top')\n",
    "plt.tight_layout()\n",
    "plt.savefig(model+'.png',dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
