{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps til at køre scriptet:\n",
    "1. Tag en kopi af denne notebook INDEN ændringer laves\n",
    "2. Skriv dit navn (martin/pratt/charlotte) i en string med små bogstaver i variablen nedenfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eksempel:\n",
    "name = \"martin\" #skal enten være: martin/pratt/charlotte\n",
    "img_path = \"/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celebA_resize3/\"\n",
    "run = \"run1_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import data_utils.data_utils_celeba_pytorch5 as data_utils\n",
    "import data_utils.network_tuning23 as network_tuning\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# Load functions\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [73,60,3]\n",
    "# Paths to data\n",
    "# Root directory for dataset\n",
    "if name==\"charlotte\":\n",
    "    dataroot = '..\\\\AlgorithmicFairness'\n",
    "    TRAIN_PATH =  dataroot + \"\\\\Data\\\\train.csv\" \n",
    "    VALID_PATH = dataroot + \"\\\\Data\\\\valid.csv\" \n",
    "    TEST_PATH = dataroot + \"\\\\Data\\\\test.csv\" \n",
    "    IMAGE_PATHS = \"C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\celebA_resize3\\\\\"\n",
    "\n",
    "else:\n",
    "    dataroot = '../AlgorithmicFairness'\n",
    "    TRAIN_PATH =  dataroot + \"/Data/train.csv\" \n",
    "    VALID_PATH = dataroot + \"/Data/valid.csv\" \n",
    "    TEST_PATH = dataroot + \"/Data/test.csv\" \n",
    "    IMAGE_PATHS = img_path\n",
    "TARGET_COL = 'Smiling'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data_train = data_utils.CelebADataset(TRAIN_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)\n",
    "data_valid = data_utils.CelebADataset(VALID_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)\n",
    "\n",
    "#tuning the network round 1\n",
    "df = pd.DataFrame(index=list(range(1,11)), columns=['layers','activations','conv_out_channels','kernel_size','conv_stride','maxpool'\\\n",
    "                                                    ,'dropout','batchnorm','optimizer','learning_rate','weight_decay','batch_size','num_epochs'\\\n",
    "                                                    ,'net','train_loss','train_accs','valid_loss','valid_accs'])\n",
    "\n",
    "df['layers'] = 1\n",
    "\n",
    "#adjust activation function\n",
    "for i in range(1,5):\n",
    "    df.at[i,'activations'] = [relu, relu, relu, relu]\n",
    "for i in range(5,11):\n",
    "    df.at[i,'activations'] = [tanh, tanh, tanh, tanh, tanh, tanh]\n",
    "\n",
    "#inserting combination of activations:\n",
    "df.iloc[0,1][1]= tanh\n",
    "df.iloc[1,1][1]= tanh\n",
    "df.iloc[6,1][1]= relu\n",
    "df.iloc[7,1][1]= relu\n",
    "\n",
    "IMAGE_SHAPE = [73,60,3]\n",
    "df['conv_out_channels'] = 16\n",
    "df['kernel_size'] = 5\n",
    "df['conv_stride'] = 1\n",
    "df['maxpool'] = 1\n",
    "df['dropout'] = 0.0\n",
    "df['batchnorm'] = False\n",
    "df['optimizer']='Adam'\n",
    "df['learning_rate']=0.001\n",
    "df['weight_decay']=0.0\n",
    "df['batch_size']=128\n",
    "df['num_epochs']=5\n",
    "\n",
    "#adjust depth\n",
    "df.at[1:4,'layers'] = 2\n",
    "df.at[7:11,'layers'] = 2\n",
    "\n",
    "#adjust channels\n",
    "df.at[1,'conv_out_channels'] = 32 \n",
    "df.at[2,'conv_out_channels'] = 64\n",
    "df.at[3,'conv_out_channels'] = 32\n",
    "df.at[4,'conv_out_channels'] = 64\n",
    "df.at[5,'conv_out_channels'] = 32\n",
    "df.at[6,'conv_out_channels'] = 128 \n",
    "df.at[7,'conv_out_channels'] = 32\n",
    "df.at[8,'conv_out_channels'] = 64\n",
    "df.at[9,'conv_out_channels'] = 32\n",
    "df.at[10,'conv_out_channels'] = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == \"martin\":\n",
    "    df = df.iloc[-3:11,:]\n",
    "elif name == \"pratt\":\n",
    "    df = df.iloc[0:3,:]\n",
    "elif name == \"charlotte\":\n",
    "    df = df.iloc[3:-3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(df.index):#range(len(df)):\n",
    "    print('Model: ', i)\n",
    "    layers = df.loc[i,'layers']\n",
    "    activations = df.loc[i,'activations']\n",
    "    conv_out_channels = df.loc[i,'conv_out_channels']\n",
    "    kernel_size = df.loc[i,'kernel_size']\n",
    "    conv_stride = df.loc[i,'conv_stride']\n",
    "    maxpool = int(df.loc[i,'maxpool'])\n",
    "    dropout = df.loc[i,'dropout']\n",
    "    batchnorm = df.loc[i,'batchnorm']\n",
    "    optimizer = df.loc[i,'optimizer']\n",
    "    learning_rate = df.loc[i,'learning_rate']\n",
    "    weight_decay = df.loc[i,'weight_decay']\n",
    "    batch_size = int(df.loc[i,'batch_size'])\n",
    "    num_epochs = df.loc[i,'num_epochs']\n",
    "    \n",
    "    net = network_tuning.tune_architecture(layers, activations, IMAGE_SHAPE, conv_out_channels, kernel_size,conv_stride, maxpool, dropout, batchnorm)\n",
    "    net_trained, df.at[i,'train_loss'], df.at[i,'train_accs'] = network_tuning.tune_train(net, data_train, optimizer, learning_rate, weight_decay, batch_size, num_epochs)\n",
    "    df.at[i,'valid_loss'], df.at[i,'valid_accs'] = network_tuning.tune_valid(net, data_valid, batch_size)\n",
    "    df.at[i,'net'] = run+name+'_model'+str(i)\n",
    "    df.to_pickle(run+name+'_df.pkl')\n",
    "    torch.save(net_trained.state_dict(), run+name+'_model'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(pd.Series(df.loc[4,'train_accs']).rolling(window=600).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(df.loc[1,'train_accs']).rolling(window=600).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
