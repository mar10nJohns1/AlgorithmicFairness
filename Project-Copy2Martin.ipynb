{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import glob\n",
    "import data_utils.data_utils_celeba_pytorch as data_utils\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "#dataroot = 'C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\AlgorithmicFairness'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('Data/list_attr_celeba.txt', sep=\" \")\n",
    "attributes.drop('Unnamed: 41',axis=1, inplace=True)\n",
    "partition = pd.read_csv('Data/list_eval_partition.txt', sep=\" \", header=None, names=['im_id','partition'])\n",
    "matched = attributes.set_index('im_id').join(partition.set_index('im_id')).replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = matched[matched['partition']==0]\n",
    "valid = matched[matched['partition']==1]\n",
    "test = matched[matched['partition']==2]\n",
    "train[1:1000].to_csv('train.csv')\n",
    "valid[1:1000].to_csv('valid.csv')\n",
    "test[1:1000].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [73,60,3]\n",
    "# Paths to data\n",
    "TRAIN_PATH =  \"Data/train.csv\" \n",
    "VALID_PATH = \"Data/valid.csv\" \n",
    "TEST_PATH = \"Data/test.csv\" \n",
    "IMAGE_PATHS = \"/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celeba/\"\n",
    "TARGET_COL = 'Smiling'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data_train = data_utils.CelebADataset(TRAIN_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)\n",
    "data_valid = data_utils.CelebADataset(VALID_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data_valid, batch_size=128,shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    images_batch, target_batch = \\\n",
    "            sample_batched['image'], sample_batched['target']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d499cf5fd9f54c5086f737459ad04c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "1 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "2 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "3 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "4 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "5 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "6 torch.Size([128, 3, 73, 60]) torch.Size([128, 2])\n",
      "7 torch.Size([104, 3, 73, 60]) torch.Size([104, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "for i_batch, sample_batched in tqdm_notebook(enumerate(dataloader)):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['target'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 10:\n",
    "        plt.figure()\n",
    "        show_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,\n",
       "         1.,  1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
       "         1., -1., -1., -1., -1., -1., -1., -1., -1.,  1., -1.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_batched['attributes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c22ecf3c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.transpose(vutils.make_grid(sample_batched['image'][5], padding=2,normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (l_out): Linear(in_features=61824, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "\n",
    "conv_out_channels =  16 # <-- Filters in your convolutional layer\n",
    "kernel_size = 5       # <-- Kernel size\n",
    "conv_stride = 1       # <-- Stride\n",
    "conv_pad    = 0       # <-- Padding\n",
    " \n",
    "\n",
    "\n",
    "conv1_h = int(height - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "conv1_w = int(width - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = int(conv_out_channels * conv1_h * conv1_w)\\\n",
    "\n",
    "# <-- Number of features concatenated before output layer\n",
    "  \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=conv_pad)\n",
    "\n",
    "             \n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x_img):\n",
    "        features = []\n",
    "        out = {}\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        #x_img = x_img.permute(0,3,1,2)\n",
    "        features_img = relu(self.conv_1(x_img))\n",
    "        \n",
    "        features_img = features_img.view(-1, features_cat_size)\n",
    "\n",
    "        ## Output layer where all features are in use ##\n",
    "        \n",
    "        out['out'] = self.l_out(features_img)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss()          #<-- Your code here.   \n",
    "\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "image = randnorm(_img_shape)\n",
    "image = image.transpose(0,3, 1, 2)\n",
    "_x_image = get_variable(Variable(torch.from_numpy(image)))\n",
    "\n",
    "# test the forward pass\n",
    "output = net(x_img=_x_image)\n",
    "output['out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, it: 5 loss: 0.00 accs: 1.00\n",
      "Valid, it: 5 loss: 0.00 accs: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Setup settings for training \n",
    "batch_size=128\n",
    "max_iter = 200\n",
    "log_every = 5\n",
    "eval_every = 5\n",
    "num_epochs = 10\n",
    "\n",
    "# Function to get label\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(batch['target']))\n",
    "\n",
    "# Function to get input\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(batch['image']))\n",
    "    }\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "# Generate batches\n",
    "batch_gen_train = DataLoader(data_train, batch_size, shuffle=True, num_workers=6)\n",
    "batch_gen_valid = DataLoader(data_valid, batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "# Train network\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch_train in enumerate(batch_gen_train):\n",
    "        if i % eval_every == 0:\n",
    "\n",
    "            # Do the validaiton\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "            for batch_valid in batch_gen_valid:\n",
    "                num = len(batch_valid['target'])\n",
    "                output = net(**get_input(batch_valid))\n",
    "                labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "                val_losses += criterion(output['out'], labels_argmax) * num\n",
    "                val_accs += accuracy(output['out'], labels_argmax) * num\n",
    "                val_lengths += num\n",
    "\n",
    "            # Divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "            valid_loss.append(get_numpy(val_losses))\n",
    "            valid_accs.append(get_numpy(val_accs))\n",
    "            valid_iter.append(i)\n",
    "    #         print(\"Valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "            net.train()\n",
    "\n",
    "        # Train network\n",
    "        output = net(**get_input(batch_train))\n",
    "        labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
    "        batch_loss = criterion(output['out'], labels_argmax)\n",
    "\n",
    "        train_iter.append(i)\n",
    "        train_loss.append(float(get_numpy(batch_loss)))\n",
    "        train_accs.append(float(get_numpy(accuracy(output['out'], labels_argmax))))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log i figure\n",
    "        if i % log_every == 0:\n",
    "            fig = plt.figure(figsize=(12,4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_iter, train_loss, label='train_loss')\n",
    "            plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(train_iter, train_accs, label='train_accs')\n",
    "            plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "            print(\"Valid, it: {} loss: {:.2f} accs: {:.4f}\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "\n",
    "        if max_iter < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, it: 7 loss: 0.00 accs: 1.00\n",
      "Valid, it: 7 loss: 0.00 accs: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "print(\"Valid, it: {} loss: {:.2f} accs: {:.4f}\".format(i, valid_loss[-1], valid_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: dimension -1 out of range of 2D tensor at ../aten/src/TH/generic/THTensor.cpp:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4e6d9bb73818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mval_male_accs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_argmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mval_male_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mval_female_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_argmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mval_female_accs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_argmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mval_female_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfemale\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1822\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: dimension -1 out of range of 2D tensor at ../aten/src/TH/generic/THTensor.cpp:41"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "val_male_losses,val_male_accs,val_male_lengths,val_female_losses,val_female_accs,val_female_lengths = 0, 0, 0, 0, 0, 0\n",
    "i=0\n",
    "for batch_valid in batch_gen_valid:\n",
    "    num = len(batch_valid)\n",
    "    output = net(**get_input(batch_valid))\n",
    "    labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "    \n",
    "    gender = batch_valid['attributes'][:,19]\n",
    "    male = np.where(gender==1)\n",
    "    female = np.where(gender==0)\n",
    "\n",
    "    val_male_losses += criterion(output['out'][male], labels_argmax[male]) * len(male[0]) \n",
    "    val_male_accs += accuracy(output['out'][male], labels_argmax[male]) * len(male[0]) \n",
    "    val_male_lengths += len(male[0])\n",
    "    val_female_losses += criterion(output['out'][female], labels_argmax[female]) * len(female[0]) \n",
    "    val_female_accs += accuracy(output['out'][female], labels_argmax[female]) * len(female[0]) \n",
    "    val_female_lengths += len(female[0])\n",
    "\n",
    "# Divide by the total accumulated batch sizes\n",
    "val_male_losses /= val_male_lengths\n",
    "val_male_accs /= val_male_lengths\n",
    "\n",
    "val_female_losses /= val_female_lengths\n",
    "val_female_accs /= val_female_lengths\n",
    "\n",
    "print(\"Valid Male, it: {} loss: {:.2f} accs: {:.4f}\\n\".format(i, val_male_losses, val_male_accs))\n",
    "print(\"Valid Female, it: {} loss: {:.2f} accs: {:.4f}\\n\".format(i, val_female_losses, val_female_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_valid['attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "d = dict()\n",
    "d['im'] = np.zeros(tuple([1000]+IMAGE_SHAPE))\n",
    "k=0\n",
    "l= [data.train['images'][i].decode() for i in  range(0,1000)]\n",
    "for j in tqdm_notebook(l):\n",
    "    im = imread(j)\n",
    "    im = resize(im,output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)\n",
    "    d['im'][k] = im\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = data.valid\n",
    "valid_new = valid\n",
    "valid_new['images'] = np.zeros(tuple([1000] + IMAGE_SHAPE), dtype='float32')\n",
    "for i, im in enumerate(data.test['images']):\n",
    "    im = imread(im.decode())\n",
    "    valid_new['images'][i] = resize(im, output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['im'] = np.zeros(tuple([1000]+IMAGE_SHAPE))\n",
    "k=0\n",
    "for j in tqdm_notebook([data.train['images'][i] for i in  range(0,1000)]):\n",
    "    im = imread(j.decode())\n",
    "    im = resize(im,output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)\n",
    "    d['im'][k] = im\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['im'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(batch_valid['images'][6]), padding=2,normalize=True).cpu(),(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = [np.where(output['out'][i]==m[i]) for i in range(0,104)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([labels_argmax[i].numpy() == pred_label[i][0][0] for i in range(0,104)]).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label[1] == labels_argmax[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [max(output['out'][i]) for i in range(0,104)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output['out'][male], labels_argmax[male])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['out'][[1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output['out'][1:2], labels_argmax[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_valid['attributes'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "\n",
    "conv_out_channels =  16 # <-- Filters in your convolutional layer\n",
    "kernel_size = 5       # <-- Kernel size\n",
    "conv_stride = 1       # <-- Stride\n",
    "conv_pad    = 0       # <-- Padding\n",
    " \n",
    "def conv_dim(dim_size):\n",
    "    int(dim_size - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "conv1_h = int(height - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "conv1_w = int(width - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = int(conv_out_channels * conv1_h * conv1_w)\\\n",
    "\n",
    "# <-- Number of features concatenated before output layer\n",
    "  \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=conv_pad)\n",
    "\n",
    "             \n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x_img):\n",
    "        features = []\n",
    "        out = {}\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        x_img = x_img.permute(0,3,1,2)\n",
    "        features_img = relu(self.conv_1(x_img))\n",
    "        \n",
    "        features_img = features_img.view(-1, features_cat_size)\n",
    "\n",
    "        ## Output layer where all features are in use ##\n",
    "        \n",
    "        out['out'] = self.l_out(features_img)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
