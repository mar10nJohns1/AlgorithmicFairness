{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import glob\n",
    "import data_utils.data_utils_celeba_pytorch as data_utils\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = 'C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\AlgorithmicFairness'\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = pd.read_csv('Data/list_attr_celeba.txt', sep=\" \")\n",
    "attributes.drop('Unnamed: 41',axis=1, inplace=True)\n",
    "partition = pd.read_csv('Data/list_eval_partition.txt', sep=\" \", header=None, names=['im_id','partition'])\n",
    "matched = attributes.set_index('im_id').join(partition.set_index('im_id')).replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = matched[matched['partition']==0]\n",
    "valid = matched[matched['partition']==1]\n",
    "test = matched[matched['partition']==2]\n",
    "train[1:1000].to_csv('train.csv')\n",
    "valid[1:1000].to_csv('valid.csv')\n",
    "test[1:1000].to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = [73,60,3]\n",
    "# Paths to data\n",
    "TRAIN_PATH =  dataroot + \"\\\\train.csv\" \n",
    "VALID_PATH = dataroot + \"\\\\valid.csv\" \n",
    "TEST_PATH = dataroot + \"\\\\test.csv\" \n",
    "IMAGE_PATHS = \"C:\\\\Users\\\\cfthe\\\\OneDrive\\\\DTU\\\\Kandidat\\\\Deep\\\\celebA\\\\\"\n",
    "TARGET_COL = 'Smiling'\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data_train = data_utils.CelebADataset(TRAIN_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)\n",
    "data_valid = data_utils.CelebADataset(VALID_PATH,IMAGE_PATHS,IMAGE_SHAPE,TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data_valid, batch_size=128,shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    images_batch, target_batch = \\\n",
    "            sample_batched['image'], sample_batched['target']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "    grid_border_size = 2\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "for i_batch, sample_batched in tqdm_notebook(enumerate(dataloader)):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['target'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 10:\n",
    "        plt.figure()\n",
    "        show_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batched['attributes'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(vutils.make_grid(sample_batched['image'][5], padding=2,normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout2d, MaxPool2d, BatchNorm2d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "\n",
    "conv_out_channels =  16 # <-- Filters in your convolutional layer\n",
    "kernel_size = 5       # <-- Kernel size\n",
    "conv_stride = 1       # <-- Stride\n",
    "conv_pad    = 0       # <-- Padding\n",
    " \n",
    "\n",
    "\n",
    "conv1_h = int(height - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "conv1_w = int(width - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = int(conv_out_channels * conv1_h * conv1_w)\\\n",
    "\n",
    "# <-- Number of features concatenated before output layer\n",
    "  \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=conv_pad)\n",
    "\n",
    "             \n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x_img):\n",
    "        features = []\n",
    "        out = {}\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        #x_img = x_img.permute(0,3,1,2)\n",
    "        features_img = relu(self.conv_1(x_img))\n",
    "        \n",
    "        features_img = features_img.view(-1, features_cat_size)\n",
    "\n",
    "        ## Output layer where all features are in use ##\n",
    "        \n",
    "        out['out'] = self.l_out(features_img)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss()          #<-- Your code here.   \n",
    "\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "image = randnorm(_img_shape)\n",
    "image = image.transpose(0,3, 1, 2)\n",
    "_x_image = get_variable(Variable(torch.from_numpy(image)))\n",
    "\n",
    "# test the forward pass\n",
    "output = net(x_img=_x_image)\n",
    "output['out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup settings for training \n",
    "batch_size=128\n",
    "max_iter = 200\n",
    "log_every = 5\n",
    "eval_every = 5\n",
    "num_epochs = 10\n",
    "\n",
    "# Function to get label\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(batch['target']))\n",
    "\n",
    "# Function to get input\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(batch['image']))\n",
    "    }\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "# Generate batches\n",
    "batch_gen_train = DataLoader(data_train, batch_size, shuffle=True, num_workers=6)\n",
    "batch_gen_valid = DataLoader(data_valid, batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "# Train network\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch_train in enumerate(batch_gen_train):\n",
    "        if i % eval_every == 0:\n",
    "\n",
    "            # Do the validaiton\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "            for batch_valid in batch_gen_valid:\n",
    "                num = len(batch_valid['target'])\n",
    "                output = net(**get_input(batch_valid))\n",
    "                labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "                val_losses += criterion(output['out'], labels_argmax) * num\n",
    "                val_accs += accuracy(output['out'], labels_argmax) * num\n",
    "                val_lengths += num\n",
    "\n",
    "            # Divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "            valid_loss.append(get_numpy(val_losses))\n",
    "            valid_accs.append(get_numpy(val_accs))\n",
    "            valid_iter.append(i)\n",
    "    #         print(\"Valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "            net.train()\n",
    "\n",
    "        # Train network\n",
    "        output = net(**get_input(batch_train))\n",
    "        labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
    "        batch_loss = criterion(output['out'], labels_argmax)\n",
    "\n",
    "        train_iter.append(i)\n",
    "        train_loss.append(float(get_numpy(batch_loss)))\n",
    "        train_accs.append(float(get_numpy(accuracy(output['out'], labels_argmax))))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log i figure\n",
    "        if i % log_every == 0:\n",
    "            fig = plt.figure(figsize=(12,4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_iter, train_loss, label='train_loss')\n",
    "            plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(train_iter, train_accs, label='train_accs')\n",
    "            plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            clear_output(wait=True)\n",
    "            #print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "\n",
    "        if max_iter < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "print(\"Valid, it: {} loss: {:.2f} accs: {:.4f}\".format(i, valid_loss[-1], valid_accs[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "val_male_losses,val_male_accs,val_male_lengths,val_female_losses,val_female_accs,val_female_lengths = 0, 0, 0, 0, 0, 0\n",
    "i=0\n",
    "for batch_valid in batch_gen_valid:\n",
    "    num = len(batch_valid)\n",
    "    output = net(**get_input(batch_valid))\n",
    "    labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "    \n",
    "    gender = batch_valid['attributes'][:,19]\n",
    "    male = np.where(gender==1)\n",
    "    female = np.where(gender==0)\n",
    "\n",
    "    val_male_losses += criterion(output['out'][male], labels_argmax[male]) * len(male[0]) \n",
    "    val_male_accs += accuracy(output['out'][male], labels_argmax[male]) * len(male[0]) \n",
    "    val_male_lengths += len(male[0])\n",
    "    val_female_losses += criterion(output['out'][female], labels_argmax[female]) * len(female[0]) \n",
    "    val_female_accs += accuracy(output['out'][female], labels_argmax[female]) * len(female[0]) \n",
    "    val_female_lengths += len(female[0])\n",
    "\n",
    "# Divide by the total accumulated batch sizes\n",
    "val_male_losses /= val_male_lengths\n",
    "val_male_accs /= val_male_lengths\n",
    "\n",
    "val_female_losses /= val_female_lengths\n",
    "val_female_accs /= val_female_lengths\n",
    "\n",
    "print(\"Valid Male, it: {} loss: {:.2f} accs: {:.4f}\\n\".format(i, val_male_losses, val_male_accs))\n",
    "print(\"Valid Female, it: {} loss: {:.2f} accs: {:.4f}\\n\".format(i, val_female_losses, val_female_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_valid['attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "d = dict()\n",
    "d['im'] = np.zeros(tuple([1000]+IMAGE_SHAPE))\n",
    "k=0\n",
    "l= [data.train['images'][i].decode() for i in  range(0,1000)]\n",
    "for j in tqdm_notebook(l):\n",
    "    im = imread(j)\n",
    "    im = resize(im,output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)\n",
    "    d['im'][k] = im\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = data.valid\n",
    "valid_new = valid\n",
    "valid_new['images'] = np.zeros(tuple([1000] + IMAGE_SHAPE), dtype='float32')\n",
    "for i, im in enumerate(data.test['images']):\n",
    "    im = imread(im.decode())\n",
    "    valid_new['images'][i] = resize(im, output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict()\n",
    "d['im'] = np.zeros(tuple([1000]+IMAGE_SHAPE))\n",
    "k=0\n",
    "for j in tqdm_notebook([data.train['images'][i] for i in  range(0,1000)]):\n",
    "    im = imread(j.decode())\n",
    "    im = resize(im,output_shape=IMAGE_SHAPE, mode='reflect', anti_aliasing=True)\n",
    "    d['im'][k] = im\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['im'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(vutils.make_grid(torch.Tensor(batch_valid['images'][6]), padding=2,normalize=True).cpu(),(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = [np.where(output['out'][i]==m[i]) for i in range(0,104)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([labels_argmax[i].numpy() == pred_label[i][0][0] for i in range(0,104)]).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label[1] == labels_argmax[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [max(output['out'][i]) for i in range(0,104)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output['out'][male], labels_argmax[male])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['out'][[1,2,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(output['out'][1:2], labels_argmax[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_valid['attributes'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "\n",
    "conv_out_channels =  16 # <-- Filters in your convolutional layer\n",
    "kernel_size = 5       # <-- Kernel size\n",
    "conv_stride = 1       # <-- Stride\n",
    "conv_pad    = 0       # <-- Padding\n",
    " \n",
    "def conv_dim(dim_size):\n",
    "    int(dim_size - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "conv1_h = int(height - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "conv1_w = int(width - kernel_size + 2 * conv_pad / conv_stride + 1)\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = int(conv_out_channels * conv1_h * conv1_w)\\\n",
    "\n",
    "# <-- Number of features concatenated before output layer\n",
    "  \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=conv_pad)\n",
    "\n",
    "             \n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x_img):\n",
    "        features = []\n",
    "        out = {}\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        x_img = x_img.permute(0,3,1,2)\n",
    "        features_img = relu(self.conv_1(x_img))\n",
    "        \n",
    "        features_img = features_img.view(-1, features_cat_size)\n",
    "\n",
    "        ## Output layer where all features are in use ##\n",
    "        \n",
    "        out['out'] = self.l_out(features_img)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
