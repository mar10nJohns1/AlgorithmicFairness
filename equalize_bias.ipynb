{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "1. import packages and assign bias attribute\n",
    "2. look at the distribution of one protected attribute\n",
    "3. run through all the attributes and asses the largest imbalance\n",
    "4. fix imbalance and check for bias\n",
    "    - 4.1 Example of augmentation\n",
    "    - 4.2 Which pictures to augment\n",
    "    - 4.3 Augmenting the right pictures\n",
    "5. Checking the number of channels\n",
    "6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. import packages and assign bias attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "plt.style.use(['seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = list(train.columns[1:41])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attr = \"Eyeglasses\"\n",
    "target = \"Smiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at the distribution of one protected attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[target,protected_attr]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only done for the groupby command in the below cell\n",
    "train[\"fill\"] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = train.groupby([protected_attr,target]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = dist.fill/train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'attr=0, target=0', 'attr=0, target=1', 'attr=1, target=0', 'attr=1, target=1'\n",
    "sizes = list(dist)\n",
    "#colors = ['blue', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, labels=labels,autopct='%1.1f%%')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "d.append([k[0]/k[1], k[2]/k[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lav en dataframe som opsummerer alle de forskellige uligheder i data for at assesse hvilke af de to der er størst ulighed på. Her kan man bruge en ratio ligesom på posteren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run trough all attributes and asses the largest imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "train_len = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = list(train.columns[1:41])\n",
    "target = \"Smiling\"\n",
    "attr.remove('Smiling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "for i in attr:\n",
    "    protected_attr = i\n",
    "    train_ = train[[target,protected_attr]]\n",
    "    train_[\"fill\"] = 9\n",
    "    dist = train_.groupby([protected_attr,target]).count()\n",
    "    dist = dist.fill/train_len\n",
    "    k = list(dist)\n",
    "    diff.append([i, k[0]/k[1], k[2]/k[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pd.DataFrame(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.columns = ['Protected_attr', 'ratio attr 0', 'ratio attr 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff.sort_values(['ratio attr 0', 'ratio attr 1'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.sort_values(['ratio attr 1'], ascending=[False]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the two heads, we that\n",
    "- 3/10 is not a part of our poster (wearing hat, lipstick, no_beard) \n",
    "- 4/10 has detected bias from our definition (gotee, pale_skin, mouth_slightly_open, high_cheekbones)\n",
    "- 3/10 has no detected bias from our definition (mustache, sideburns, attractive) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fix imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to balance the unbalanced training set. Looking at the attribute with the largest imbalance, `high_cheekbones` (were our metric also detected a bias), we see that the `ratio_attr_0` is 4.8 and `ratio_attr_1` is 0.17. \n",
    "\n",
    "The ratio formula is essentially\n",
    "$$\n",
    "ratio\\_attr\\_0 = \\frac{attr = 0, target = 0}{attr = 0, target = 1}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "ratio\\_attr\\_1 = \\frac{attr = 1, target = 0}{attr = 1, target = 1}\n",
    "$$\n",
    "\n",
    "This mans that \n",
    "- out of all the people NOT having high cheeckbones, 4.8 times as many were not smiling than smiling \n",
    "- out of all the people HAVING high cheeckbones, many more were smiling than not smiling\n",
    "\n",
    "and hence this particular training data is very unbbalanced.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the following article, https://towardsdatascience.com/deep-learning-unbalanced-training-data-solve-it-like-this-6c528e9efea6, we have three absic approaches (more complex ones exist of course): \n",
    "\n",
    "1. Undersampling- Randomly delete the class which has sufficient observations so that the comparative ratio of two classes is significant in our data.Although this approach is really simple to follow but there is a high possibility that the data that we are deleting may contain important information about the predictive class.\n",
    "2. Oversampling-For the unbalanced class randomly increase the number of observations which are just copies of existing samples.This ideally gives us sufficient number of samples to play with.The oversampling may lead to overfitting to the training data\n",
    "3. Synthetic sampling(SMOTE)-The technique asks to synthetically manufacture observations of unbalanced classes which are similar to the existing using nearest neighbors classification.The problem is what to do when the number of observations of is an extremely rare class .For example-we may have only one picture of a rare species which we want to identify using image classification algorithm\n",
    "\n",
    "I am using option 2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Example of augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open('/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celebA_resize3/000001.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=im.convert(\"RGB\")\n",
    "r,g,b=im.split()\n",
    "r=r.convert(\"RGB\")\n",
    "g=g.convert(\"RGB\")\n",
    "b=b.convert(\"RGB\")\n",
    "#im_blur=im.filter(ImageFilter.GaussianBlur)\n",
    "im_unsharp=im.filter(ImageFilter.UnsharpMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_unsharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We choose only to use the im_unsharp and the b picture together with the normal pictur\n",
    "# which means, that we are augmenting 3 pictures every time we upsample 1 time\n",
    "pictures_upsampling = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Which pictures to augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we in this particular example, want to upsample the number of smiling people with high cheekbones and the number of non-smiling people with high cheekbones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assessing the number of pictures that are different\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "train_len = len(train)\n",
    "attr = list(train.columns[1:41])\n",
    "protected_attr = \"High_Cheekbones\"\n",
    "target = \"Smiling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining train as only the target and the protected attribute\n",
    "train = train[[target,protected_attr]]\n",
    "#This is only done for the groupby command in the below cell\n",
    "train[\"fill\"] = 9\n",
    "#Groupby and counting the number of occurances\n",
    "dist = train.groupby([protected_attr,target]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'attr=0, target=0', 'attr=0, target=1', 'attr=1, target=0', 'attr=1, target=1'\n",
    "sizes = list(dist.fill)\n",
    "#colors = ['blue', 'yellowgreen', 'lightcoral', 'lightskyblue']\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, labels=labels,autopct='%1.1f%%')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only training data\n",
    "sum(dist.fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the table above, we could potentially also downsample, however, we would loose a lot of information. We need to upsample around 60,000 pictures of smiling people without high cheeckbone (smiling_not_highcheekbones = `s_n_hc`), and around 50,000 pictures of non-smiling people with high cheekbones (nonsmiling_highcheekbones = `ns_hc`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all the attributes for every single image and which partition it belongs to:\n",
    "pp = pd.read_csv('Data/list_attr_celeba.txt', sep= \" \")\n",
    "part = pd.read_csv('Data/list_eval_partition.txt', sep= \" \",header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naming columns in the partition dataset\n",
    "part.columns = ['im_id','partition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pp.merge(part, how = 'left', on = 'im_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pp[pp.partition == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have filtered the test data out of the sample\n",
    "pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND it is equal to the amount of datapoints in the groupby table\n",
    "sum(dist.fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of the two different characteristics that we want to obtain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n_hc = pp[(pp[target] == 1) & (pp[protected_attr] == -1)].im_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that this number is the same as in the dist table above\n",
    "len(s_n_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_hc = pp[(pp[target] == -1) & (pp[protected_attr] == 1)].im_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that this number is the same as in the dist table above\n",
    "len(ns_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Augmenting the right pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring that we are subtracting the right numbers. It is always the differece between\n",
    "# the two attributes we want:\n",
    "if dist.fill[0][0]>dist.fill[0][1]:\n",
    "    s_n_hc_count = dist.fill[0][0]-dist.fill[0][1]\n",
    "else:\n",
    "    s_n_hc_count = dist.fill[0][1]-dist.fill[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring that we are subtracting the right numbers. It is always the differece between\n",
    "# the two attributes we want:\n",
    "if dist.fill[1][1]>dist.fill[1][0]:\n",
    "    ns_hc_count = dist.fill[1][1]-dist.fill[1][0]\n",
    "else: \n",
    "    ns_hc_count = dist.fill[1][0]-dist.fill[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We need to get',s_n_hc_count,'number of smiling non HC-people, and',ns_hc_count,'numbber of non-similing HC-people. \\n')\n",
    "print('As we can make', pictures_upsampling, 'different augmentations of every image, that means we need to random sample',\\\n",
    "     s_n_hc_count/pictures_upsampling,'smiling non HC-people, and',ns_hc_count/pictures_upsampling\\\n",
    "      ,'non-similing HC-people.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling = [list(s_n_hc) , list(ns_hc)]\n",
    "s_n_hc_up = int(np.round(s_n_hc_count/pictures_upsampling))\n",
    "ns_hc_up = int(np.round(ns_hc_count/pictures_upsampling))\n",
    "range_ = [s_n_hc_up , ns_hc_up]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that this is equal to the print statement above:\n",
    "s_n_hc_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that this is equal to the print statement above:\n",
    "ns_hc_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = '/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celebA_resize3/'\n",
    "saveto_root = '/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celebA_resize3_aug_HC/'\n",
    "train = pd.read_csv(\"Data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = copy.deepcopy(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Writing pictures \n",
    "for i in range(2):\n",
    "    augment = upsampling[i]\n",
    "    rang = range_[i]\n",
    "    for k in tqdm(range(rang)):\n",
    "        image = random.randrange(len(augment))\n",
    "        image = augment[image]\n",
    "        im=Image.open(img_root+image)\n",
    "        im_=im.convert(\"RGB\")\n",
    "        r,g,b=im_.split()\n",
    "        r=r.convert(\"RGB\")\n",
    "        g=g.convert(\"RGB\")\n",
    "        b=b.convert(\"RGB\")\n",
    "        im_unsharp=im_.filter(ImageFilter.UnsharpMask)\n",
    "\n",
    "        #r.save(saveto_root+'r_'+str(k)+\"_\"+image)\n",
    "        #g.save(saveto_root+'g_'+str(k)+\"_\"+image)\n",
    "        \n",
    "        #Saving image, b, and unsharp:\n",
    "        im.save(saveto_root+'im_'+str(k)+\"_\"+image)#Added\n",
    "        b.save(saveto_root+'b_'+str(k)+\"_\"+image)\n",
    "        im_unsharp.save(saveto_root+'un_'+str(k)+\"_\"+image)\n",
    "        \n",
    "        \n",
    "        traindf = traindf.append([train[train.im_id==image]]*pictures_upsampling\\\n",
    "                                 ,ignore_index=True)\n",
    "        \n",
    "        traindf.iloc[-1,0] = 'un_'+str(k)+\"_\"+image\n",
    "        traindf.iloc[-2,0] = 'b_'+str(k)+\"_\"+image\n",
    "        traindf.iloc[-3,0] = 'im_'+str(k)+\"_\"+image\n",
    "        \n",
    "traindf.to_csv('Data/train_augmented_'+protected_attr+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"Data/train_augmented_mouth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = traindf[[target,protected_attr]]\n",
    "#This is only done for the groupby command in the below cell\n",
    "train[\"fill\"] = 9\n",
    "dist = train.groupby([protected_attr,target]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check out many pictures is in the folder:\n",
    "import os\n",
    "files = os.listdir(saveto_root)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when we would like to have \n",
    "sum(range_)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added also has this many pictures - thises all needs to be added to the dataframe, traindf\n",
    "len(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving pictures as pickle\n",
    "with open(\"Data/pictures_augmented.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(added, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading pickle with list of photos that ahs been augmented\n",
    "with open(\"Data/pictures_augmented.txt\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the method defined for the training of the models needs a data-frame where it reads from the `im_id` columns in order to find the picture, we need to add all these pictures with their attributes to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img in enumerate(added):\n",
    "    #print(img)\n",
    "    if i%10000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in added:\n",
    "    traindf = traindf.append(traindf[traindf.im_id==img])\n",
    "    traindf = traindf.append(traindf[traindf.im_id==img])\n",
    "    traindf = traindf.append(traindf[traindf.im_id==img])\n",
    "    traindf = traindf.append(traindf[traindf.im_id==img])\n",
    "    \n",
    "    traindf.iloc[-1,0] = str(k)+'r_'+img\n",
    "    traindf.iloc[-2,0] = str(k)+'g_'+img\n",
    "    traindf.iloc[-3,0] = str(k)+'b_'+img\n",
    "    traindf.iloc[-4,0] = str(k)+'un_'+img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(traindf)-len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have sampled the images so that we have the correct amount of each class! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checking the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im=Image.open('/Users/MartinJohnsen/Documents/Martin Johnsen/MMC/3. Semester/Deep Learning/Projects/Algorithmic fairness/Data/celebA_resize3_aug/0r_057136.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#channels\n",
    "len(im.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.getbands()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Happens on AWS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
